{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple multi-layered feedforward backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from math import floor, ceil\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "pd.options.display.float_format = '{:,.6f}'.format\n",
    "sys.path.append(\"c:\\python38\\lib\\site-packages\")\n",
    "sns.set(style=\"whitegrid\")\n",
    "THEME = \"darkslategray\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backprop on the Seeds Dataset\n",
    "from random import random\n",
    "from math import exp\n",
    " \n",
    "#Initialize a network\n",
    "#def initialize_network(n_inputs, n_hidden, n_outputs):\n",
    "#    network = list()\n",
    "#    hidden_layer = [{'weights':[random() for i in range(n_inputs + 1)]} for i in range(n_hidden)]\n",
    "#    network.append(hidden_layer)\n",
    "#    output_layer = [{'weights':[random() for i in range(n_hidden + 1)]} for i in range(n_outputs)]\n",
    "#    network.append(output_layer)\n",
    "#    return network\n",
    "\n",
    "#Initialize a network\n",
    "def initialize_network(n_inputs, n_hidden, n_outputs):\n",
    "    network = list() #network is a list\n",
    "    \n",
    "    hidden_layer = [{'weights':[0.24, 0.88]}]\n",
    "    network.append(hidden_layer)\n",
    "    \n",
    "    output_layer = [{'weights':[0.24, 0.88]}]\n",
    "    network.append(output_layer)\n",
    "    return network\n",
    "\n",
    "def calc_activity(inputs, weights, bias):\n",
    "    \"\"\"\n",
    "    Activity function to calculate the activity of a neuron given weights and inputs\n",
    "    \"\"\"\n",
    "    weighting_sum = weights[-1]\n",
    "    for i in range(len(weights)-1):\n",
    "        weighting_sum += weights[i] * inputs[i]\n",
    "    return weighting_sum\n",
    "\n",
    "def calc_activation(weighting_sum):\n",
    "    \"\"\"\n",
    "    Activation function activates the activity of the neuron. The activation function\n",
    "    used in this case in the sigmoid function\n",
    "    \"\"\"\n",
    "    try:\n",
    "        sigmoid = 1.0 / (1.0 + exp(-weighting_sum))\n",
    "    except OverflowError: #if extreme float values\n",
    "        sigmoid = float('inf')\n",
    "    return sigmoid\n",
    "\n",
    "def softmax(z):\n",
    "    e = np.exp(r - np.max(z))\n",
    "    return e / e.sum(axis = 0)\n",
    "\n",
    "# Calculate the derivative of an neuron output\n",
    "def transfer_derivative(output):\n",
    "    return output * (1.0 - output)\n",
    "\n",
    "## Forward propagate input to a network output\n",
    "def forward_propagate(network, row):\n",
    "    inputs = row\n",
    "    for layer in network:\n",
    "        new_inputs = []\n",
    "        for neuron in layer:\n",
    "            activity = calc_activity(inputs, neuron['weights'], bias)\n",
    "            neuron['output'] = calc_activation(activity)\n",
    "            new_inputs.append(neuron['output'])\n",
    "        inputs = new_inputs\n",
    "    return inputs\n",
    "\n",
    "# Backpropagate error and store in neurons\n",
    "def backward_propagate_error(network, target):\n",
    "    for i in reversed(range(len(network))):\n",
    "        layer = network[i]\n",
    "        errors = list()\n",
    "        if i != len(network)-1: #If hidden layer\n",
    "            for j in range(len(layer)): #each node j in this hidden layer\n",
    "                error = 0.0\n",
    "                for neuron in network[i + 1]:\n",
    "                    error += (neuron['weights'][j] * neuron['delta'])\n",
    "                errors.append(error)\n",
    "        else: # If output layer\n",
    "            for j in range(len(layer)): #each node j in this outer layer\n",
    "                neuron = layer[j]\n",
    "                errors.append(target[j] - neuron['output'])\n",
    "        for j in range(len(layer)): #delta for each node j layer\n",
    "            neuron = layer[j]\n",
    "            neuron['delta'] = errors[j] * transfer_derivative(neuron['output'])\n",
    "            \n",
    "#Backpropagation Algorithm With Stochastic Gradient Descent \n",
    "def stochastic_gd(network, train, NUM_OUTPUTS, EPOCHS, LEARN_RATE):\n",
    "    train_errors= []\n",
    "    for epoch in range(int(EPOCHS)):\n",
    "        first_row = True\n",
    "        for row in train:\n",
    "            target = [0 for _ in range(int(NUM_OUTPUTS))]\n",
    "            target[int(row[-1])] = 1\n",
    "            forward_propagate(network, row)\n",
    "            backward_propagate_error(network, target)    \n",
    "            update_weights(network, row, LEARN_RATE)            \n",
    "        train_errors.append(error_rate(network, train))\n",
    "    return network, train_errors\n",
    "\n",
    "#Update network weights with error\n",
    "def update_weights(network, row, l_rate):    \n",
    "    for i in range(len(network)):        \n",
    "        if i != 0: #if not the first layer\n",
    "            inputs = [neuron['output'] for neuron in network[i - 1]]\n",
    "        else:\n",
    "            inputs = row[:-1] #if the firt layer, use the inputs\n",
    "        for neuron in network[i]:\n",
    "            for j in range(len(inputs)):\n",
    "                neuron['weights'][j] += l_rate * neuron['delta'] * inputs[j]\n",
    "            neuron['weights'][-1] += l_rate * neuron['delta']\n",
    "\n",
    "#Make a prediction with a network\n",
    "def predict(network, row):\n",
    "    outputs = forward_propagate(network, row)\n",
    "    return outputs.index(max(outputs))\n",
    "\n",
    "def error_rate(network, data):\n",
    "    correct = 0\n",
    "    for row in data:\n",
    "        if predict(network, row) == float(row[-1]):\n",
    "            correct += 1\n",
    "    return (1-(correct / float(len(data)))) * 100.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run your code to determine the initial activation function value (do not update the weights) using the following weights for each of two input values respectively: Weights are [0.24, 0.88]; Inputs are [0.8, 0.9], the desired output is 0.95, bias = 0 and eta = 5.0.  What is the activation value after this iteration?  Answer to 4 significant decimal digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[{'weights': [0.24, 0.88]}], [{'weights': [0.24, 0.88]}]]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'float' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-40a203ec3ff2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mNETWORK\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minitialize_network\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mNUM_INPUTS\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mLAYERS\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNUM_OUTPUTS\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mNETWORK\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[0mNN0\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstochastic_gd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mNETWORK\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m0.8\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.9\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNUM_OUTPUTS\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mEPOCHS\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mLEARN_RATE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-20-59f9954064cc>\u001b[0m in \u001b[0;36mstochastic_gd\u001b[1;34m(network, train, NUM_OUTPUTS, EPOCHS, LEARN_RATE)\u001b[0m\n\u001b[0;32m     89\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m             \u001b[0mtarget\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mNUM_OUTPUTS\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 91\u001b[1;33m             \u001b[0mtarget\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m             \u001b[0mforward_propagate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrow\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m             \u001b[0mbackward_propagate_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'float' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':    \n",
    "    LAYERS = 1\n",
    "    EPOCHS = 1\n",
    "    LEARN_RATE =  5\n",
    "    NUM_INPUTS = 2\n",
    "    NUM_OUTPUTS = 1   \n",
    "    NETWORK = initialize_network(NUM_INPUTS, LAYERS, NUM_OUTPUTS) \n",
    "    print(NETWORK)\n",
    "    NN0 = stochastic_gd(NETWORK, [0.8, 0.9], NUM_OUTPUTS, EPOCHS, LEARN_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Now restart your program with the same initial conditions and perform 75 iterations where you update the weights and the bias.  What is the activation function value now?  Remember, this activation function value is computed after the 75th weight/bias update.  Again, answer to 4 significant decimal digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this question, use the same initial values as to inputs, weights, eta, but change the desired output to 0.15.  Perform the Perceptron Delta Function to update the weights and do this for 30 iterations.  What is the activation function value after the 30th iteration?  Remember, each iteration encompasses updating the weights.  Thus, the actual output must be based on the 30th weight update after which the inputs are fed forward thru the network to produce an activation function value.  Answer to 4 decimal digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One can consider the bias theta as a weight with a corresponding input value fixed at 1.  If we want to update this \"weight\", i.e., the bias, we can apply the same methodology in determining fraction numerator partial differential E over denominator partial differential theta end fraction in the Method of Steepest Descent (MOSD) when using the Sigmoid Activation function.  If our Perceptron has a single input value of x space equals space 2 and an activation value of y space equals space 0.3 and desired output of 0.4, what is the value of fraction numerator partial differential E over denominator partial differential theta end fraction?  To answer this correctly, derive the value of fraction numerator partial differential E over denominator partial differential theta end fraction.  Answer to 3 significant decimal digits."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
