{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Marktus Atanga__\n",
    "\n",
    "__EN.605.647.8VL2.FA21 Neural Networks__\n",
    "\n",
    "__Module 5__\n",
    "\n",
    "__Simple multi-layered feedforward backpropagation__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from math import exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_network():\n",
    "    network = list() #initialize network is a list\n",
    "    hidden_layer = [{'weights': [0.24, 0.88, 0.0]}, {'weights': [0.24, 0.88, 0.0]}] \n",
    "    output_layer = [{'weights': [0.24, 0.88, 0]}]\n",
    "    network.append(hidden_layer)\n",
    "    network.append(output_layer)\n",
    "    return network\n",
    "\n",
    "def calc_activity(inputs, weights):\n",
    "    weighting_sum = weights[-1] #last weight is the bias\n",
    "    for i in range(len(weights)-1):\n",
    "        weighting_sum += weights[i] * inputs[i]\n",
    "    return weighting_sum\n",
    "\n",
    "def calc_activation(weighting_sum):\n",
    "    try:\n",
    "        sigmoid = 1.0 / (1.0 + exp(-weighting_sum))\n",
    "    except OverflowError: #if extreme float values\n",
    "        sigmoid = float('inf')\n",
    "    return sigmoid\n",
    "\n",
    "def forward_propagate(network, inputs):\n",
    "    for layer in network:\n",
    "        new_inputs = []\n",
    "        for neuron in layer:\n",
    "            activity = calc_activity(inputs, neuron['weights'])\n",
    "            neuron['output'] = calc_activation(activity)\n",
    "            new_inputs.append(neuron['output'])\n",
    "        inputs = new_inputs\n",
    "    return inputs\n",
    "\n",
    "def backward_propagate_error(network, target):\n",
    "    for i in reversed(range(len(network))):\n",
    "        layer = network[i]\n",
    "        errors = list()\n",
    "        if i != len(network)-1: #If hidden layer\n",
    "            for j in range(len(layer)): #each node j in hidden layer\n",
    "                error = 0.0\n",
    "                for neuron in network[i + 1]:\n",
    "                    error += (neuron['weights'][j] * neuron['delta'])\n",
    "                errors.append(error)\n",
    "        else: # If output layer\n",
    "            for j in range(len(layer)): #each node j in this outer layer\n",
    "                neuron = layer[j]\n",
    "                errors.append(target - neuron['output'])\n",
    "        for j in range(len(layer)): #delta for each node j of a layer\n",
    "            neuron = layer[j]\n",
    "            neuron['delta'] = errors[j] * (1.0 - neuron['output'])* neuron['output']\n",
    "    \n",
    "def update_weights(network, row, l_rate):    \n",
    "    for i in range(len(network)):        \n",
    "        if i != 0: #if not the first layer\n",
    "            inputs = [neuron['output'] for neuron in network[i - 1]]\n",
    "        else:\n",
    "            inputs = row[:-1] #if the firt layer, use the inputs\n",
    "        for neuron in network[i]:\n",
    "            for j in range(len(inputs)):\n",
    "                neuron['weights'][j] += l_rate * neuron['delta'] * inputs[j]\n",
    "            neuron['weights'][-1] += l_rate * neuron['delta']\n",
    "    \n",
    "def train_nn(network, train, NUM_OUTPUTS, EPOCHS, LEARN_RATE):\n",
    "    for epoch in range(int(EPOCHS)):\n",
    "        target = train[-1]\n",
    "        train_inputs = train[:-1] #keep everything except the last value which is the target\n",
    "        forward_propagate(network, train_inputs)\n",
    "        backward_propagate_error(network, target)    \n",
    "        update_weights(network, train_inputs, LEARN_RATE)            \n",
    "    return network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___1___: Run your code to determine the initial activation function value (do not update the weights) using the following weights for each of two input values respectively: Weights are [0.24, 0.88]; Inputs are [0.8, 0.9], the desired output is 0.95, bias = 0 and eta = 5.0.  What is the activation value after this iteration?  Answer to 4 significant decimal digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7279011823597308"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "activity = calc_activity([0.8, 0.9, 0.95], [0.24, 0.88, 0])\n",
    "calc_activation(activity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__2__: Now restart your program with the same initial conditions and perform 75 iterations where you update the weights and the bias.  What is the activation function value now?  Remember, this activation function value is computed after the 75th weight/bias update.  Again, answer to 4 significant decimal digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'weights': [0.3172119447475883, 0.88, 0.09651493093448521],\n",
       "   'output': 0.7580609317427667,\n",
       "   'delta': 2.519875959740569e-05},\n",
       "  {'weights': [0.4022066463839037, 0.88, 0.2027583079798795],\n",
       "   'output': 0.7885493752644813,\n",
       "   'delta': 3.9039006974718034e-05}],\n",
       " [{'weights': [0.929111532901536, 1.5829148886507758, 0.9330859380884207],\n",
       "   'output': 0.947049338241104,\n",
       "   'delta': 0.0001479665082256793}]]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if __name__ == '__main__':    \n",
    "    NUM_HIDEN_LAYERS = 1\n",
    "    EPOCHS = 75\n",
    "    LEARN_RATE =  5\n",
    "    NUM_INPUTS = 2\n",
    "    NUM_OUTPUTS = 1 \n",
    "    network = init_network()    \n",
    "    NN0 = train_nn(network, [0.8, 0.9, 0.95], NUM_OUTPUTS, EPOCHS, LEARN_RATE)\n",
    "NN0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__3__. For this question, use the same initial values as to inputs, weights, eta, but change the desired output to 0.15.  Perform the Perceptron Delta Function to update the weights and do this for 30 iterations.  What is the activation function value after the 30th iteration?  Remember, each iteration encompasses updating the weights.  Thus, the actual output must be based on the 30th weight update after which the inputs are fed forward thru the network to produce an activation function value.  Answer to 4 decimal digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'weights': [0.2561993624293293, 0.88, 0.020249203036661643],\n",
       "   'output': 0.7344193778714624,\n",
       "   'delta': 5.7224534938731525e-06},\n",
       "  {'weights': [0.12651711886898717, 0.88, -0.14185360141376593],\n",
       "   'output': 0.6794749984367859,\n",
       "   'delta': -7.78754490259478e-09}],\n",
       " [{'weights': [-0.6635299877607927,\n",
       "    0.0006582398735328135,\n",
       "    -1.2454686564858446],\n",
       "   'output': 0.15034622068188852,\n",
       "   'delta': -4.42270035721402e-05}]]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if __name__ == '__main__':    \n",
    "    NUM_HIDEN_LAYERS = 1\n",
    "    EPOCHS = 30\n",
    "    LEARN_RATE =  5\n",
    "    NUM_INPUTS = 2\n",
    "    NUM_OUTPUTS = 1   \n",
    "    network = init_network()    \n",
    "    NN1 = train_nn(network, [0.8, 0.9, 0.15], NUM_OUTPUTS, EPOCHS, LEARN_RATE)\n",
    "NN1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__4__. One can consider the bias theta as a weight with a corresponding input value fixed at 1.  If we want to update this \"weight\", i.e., the bias, we can apply the same methodology in determining fraction numerator partial differential E over denominator partial differential theta end fraction in the Method of Steepest Descent (MOSD) when using the Sigmoid Activation function.  If our Perceptron has a single input value of x space equals space 2 and an activation value of y space equals space 0.3 and desired output of 0.4, what is the value of fraction numerator partial differential E over denominator partial differential theta end fraction?  To answer this correctly, derive the value of fraction numerator partial differential E over denominator partial differential theta end fraction.  Answer to 3 significant decimal digits."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
